# AutoWebCompat - Automatically detect web compatibility issues

[![Build Status](https://travis-ci.org/marco-c/autowebcompat.svg?branch=master)](https://travis-ci.org/marco-c/autowebcompat)

The aim of this project is creating a tool to automatically detect [web compatibility issues][] without human intervention.

[web compatibility issues]: https://wiki.mozilla.org/Compatibility#What_is_Web_Compatibility

### Collecting screenshots

The project uses Selenium to collect web page screenshots automatically on Firefox and Chrome.

The crawler loads web pages from the URLs on the [webcompat.com tracker](https://webcompat.com/) and tries to reproduce the reported issues by interacting with the elements of the page. As soon as the page is loaded and after every interaction with the elements, the crawler takes a screenshot.

The crawler repeats the same steps in Firefox and Chrome, generating a set of comparable screenshots.

The `data/` directory contains the screenshots generated by the crawler (N.B.: This directory is not present in the repository itself, but it will be created automatically after you setup the project as described in the **Setup** paragraph).

### Labeling
[Labeling Guide](LABELING.md)

### Training

Now that we have a dataset with labels, we can train a neural network to automatically detect screenshots that are incompatible. We are currently using a [Siamese architecture](https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf) with different Convolutional Neural Networks, but are open to test other ideas.

We plan to employ three training methodologies:
1. Training from scratch on the entire training set;
2. Finetuning a network previously pretrained on ImageNet (or other datasets);
3. Finetuning a network previously pretrained in an unsupervised fashion.

For the unsupervised training, we are using a related problem for which we already have labels (detecting screenshots belonging to the same website). The pre-training can be helpful because we have plenty of data (as we don't need to manually label them) and we can fine-tune the network we pre-train for our problem of interest.


## Structure of the project

TODO

## Setup

**Python 3** is required.

- Install [Git Large File Storage](https://git-lfs.github.com/), either manually or through a package like `git-lfs` if available on your system (in case of using [PackageCloud](https://github.com/git-lfs/git-lfs/blob/master/INSTALLING.md)).
- Clone the repository with submodules: `git lfs clone --recurse-submodules REPO_URL`
- Set up Nomad, Consul and Vault clusters with the following Nomad node meta tags:
  - `storage_path` on the node(s) that shuold run data store services (databases, object store)
  - `code_path` on the node(s) that have the project source code
  - `crawler`, to be set to `true` on machines that should run browsers

TODO

## Communication

Real-time communication for this project happens on [Mozilla's IRC network](https://wiki.mozilla.org/IRC), irc.mozilla.org, in the #webcompat channel.
